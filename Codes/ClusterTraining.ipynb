{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Renganathan\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from scipy.misc import imread\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import tensorflow as tf\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import lightgbm as gbm\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import pickle\n",
    "%matplotlib nbagg\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('TrainDataScaledPCA.csv', index_col=0)\n",
    "X = data.drop('default_ind', axis=1)\n",
    "y = data['default_ind']\n",
    "\n",
    "cluster_model = KMeans(n_clusters=2, n_jobs=-1, verbose=100, random_state=7)\n",
    "cluster_model.fit(X)\n",
    "cluster_y = cluster_model.predict(X)\n",
    "\n",
    "pickle.dump(cluster_model, open('clustering.sav', 'wb'))\n",
    "\n",
    "train_data1 = data.iloc[np.where(cluster_y == 0)[0]]\n",
    "train_data2 = data.iloc[np.where(cluster_y == 1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent in Cluster 1 = 19.117 %\n",
      "Percent in Cluster 2 = 80.883 %\n",
      "Percent 1 in Cluster 1 12.496 %\n",
      "Percent 1 in Cluster 2 32.542 %\n"
     ]
    }
   ],
   "source": [
    "print('Percent in Cluster 1 =', round(len(train_data1)/len(data)*100, 3), '%')\n",
    "print('Percent in Cluster 2 =', round(len(train_data2)/len(data)*100, 3), '%')\n",
    "\n",
    "print('Percent 1 in Cluster 1', round(sum(train_data1['default_ind']==1)/len(train_data1)*100, 4), '%')\n",
    "print('Percent 1 in Cluster 2', round(sum(train_data2['default_ind']==1)/len(train_data2)*100, 4), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_data, t, n, max_depth, bf, ff, models):\n",
    "\n",
    "    models_XGB_LB = []\n",
    "\n",
    "    # Parameters to be input directly\n",
    "\n",
    "    model_name = ['LightBoost', 'AdaBoost']\n",
    "\n",
    "    n_undersamples = len(models) \n",
    "    i = 0\n",
    "\n",
    "    for i in range(n_undersamples):\n",
    "\n",
    "        pos_class = train_data[train_data['default_ind']==1]\n",
    "        neg_class = train_data[train_data['default_ind']==0]\n",
    "        neg_resampled = neg_class.sample(n= int(1 * len(pos_class)), replace=False)\n",
    "\n",
    "        train_data_resampled = pd.concat([pos_class, neg_resampled])\n",
    "\n",
    "        # Use these for Trainings\n",
    "\n",
    "        X_train_resampled = train_data_resampled.drop('default_ind', axis=1)\n",
    "        y_train_resampled = train_data_resampled['default_ind']    \n",
    "\n",
    "        # Under Sampling Negative Class\n",
    "        print(round(i/n_undersamples*100, 3), '% Done')\n",
    "        print('\\n\\nt =', t[i], 'Number of Estimators =', n[i], 'Depth =', max_depth[i], 'Model =', model_name[models[i]])\n",
    "\n",
    "        #Training\n",
    "\n",
    "        if models[i] == 1:\n",
    "\n",
    "            #XGBoost\n",
    "            model_XGB = XGBClassifier(n_estimators = n[i] , learning_rate=0.1, max_depth=max_depth[i] ,reg_lambda=0.01, n_jobs = -1, verbose =100) \n",
    "            model_XGB.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "            y_hat_XGB = model_XGB.predict(X_train_resampled)\n",
    "            print('\\nTrain Score XGB =', accuracy_score(y_train_resampled, y_hat_XGB))\n",
    "            print('Train F1 Score XGB =', f1_score(y_train_resampled, y_hat_XGB))    \n",
    "            print('Train Precision XGB=', precision_score(y_train_resampled, y_hat_XGB))\n",
    "            print('Train Recall XGB=', recall_score(y_train_resampled, y_hat_XGB))\n",
    "\n",
    "            models_XGB_LB.append(model_XGB)\n",
    "\n",
    "        else:\n",
    "\n",
    "            #LightBoost\n",
    "            cv_params = {\n",
    "            'max_depth': max_depth[i],\n",
    "            'objective': 'binary',\n",
    "            'metric':'auc',  \n",
    "            'feature_fraction': ff[i], \n",
    "            'bagging_fraction': bf[i],\n",
    "            'reg_lambda': 1,\n",
    "            'n_estimators': n[i]\n",
    "            }\n",
    "\n",
    "            gbm_train = gbm.Dataset(X_train_resampled, y_train_resampled)\n",
    "            model_LB = gbm.train(cv_params,  \n",
    "                        gbm_train,\n",
    "                        verbose_eval=1)\n",
    "\n",
    "            y_train_prob = model_LB.predict(X_train_resampled)\n",
    "            thres = t[i]\n",
    "            y_pred_train = np.zeros(len(y_train_prob))\n",
    "            y_pred_train[np.argwhere(y_train_prob>thres)] = 1\n",
    "            print('\\nTrain Score LB =', accuracy_score(y_train_resampled, y_pred_train))\n",
    "            print('Train F1 Score LB =', f1_score(y_train_resampled, y_pred_train))\n",
    "            print('Train Precision LB=', precision_score(y_train_resampled, y_pred_train))\n",
    "            print('Train Recall LB=', recall_score(y_train_resampled, y_pred_train))\n",
    "\n",
    "            models_XGB_LB.append(model_LB)\n",
    "            \n",
    "    return models_XGB_LB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % Done\n",
      "\n",
      "\n",
      "t = 0.55 Number of Estimators = 100 Depth = 2 Model = LightBoost\n",
      "\n",
      "Train Score LB = 0.7175902389425521\n",
      "Train F1 Score LB = 0.6725611553197761\n",
      "Train Precision LB= 0.800140252454418\n",
      "Train Recall LB= 0.5800711743772242\n",
      "0.0 % Done\n",
      "\n",
      "\n",
      "t = 0.45 Number of Estimators = 700 Depth = 2 Model = LightBoost\n",
      "\n",
      "Train Score LB = 0.7260877589627648\n",
      "Train F1 Score LB = 0.7408151236656553\n",
      "Train Precision LB= 0.703016241299304\n",
      "Train Recall LB= 0.7829096110367738\n"
     ]
    }
   ],
   "source": [
    "# t1 = [0.5, 0.55, 0.5, 0.55, 0.45, 0.5, 0.5, 0.55, 0.45, 0.55]\n",
    "# n1 = [50, 100, 300, 100, 75, 50, 50, 75, 200, 300]\n",
    "# max_depth1 = [2, 2, 2, 2, 2, 2, 4, 3, 1, 2]\n",
    "# models1 = [0]*10\n",
    "# bf1 = [0.8, 0.8, 1, 1, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9]\n",
    "# ff1 = [0.8, 0.8, 0.8, 0.9, 0.8, 1, 0.9, 1, 0.8, 0.8]\n",
    "\n",
    "t1 = [0.55]\n",
    "n1 = [100]\n",
    "max_depth1 = [2]\n",
    "models1 = [0]\n",
    "bf1 = [0.8]\n",
    "ff1 = [0.8]\n",
    "\n",
    "models1 = train(train_data1, t1, n1, max_depth1, bf1, ff1, models1)\n",
    "pickle.dump(models1, open('Dummy/models11.sav', 'wb'))\n",
    "pickle.dump(t1, open('Dummy/threshold11.sav', 'wb'))\n",
    "\n",
    "# t2 = [0.5, 0.5, 0.45, 0.45, 0.45, 0.45, 0.5, 0.5, 0.5, 0.5]\n",
    "# n2 = [200, 75, 700, 100, 75, 50, 50, 75, 300, 75]\n",
    "# max_depth2 = [3, 4, 2, 10, 10, 10, 4, 15, 3, 7]\n",
    "# models2 = [0]*10\n",
    "# bf2 = [0.9, 0.9, 0.8, 1, 0.9, 0.8, 0.9, 0.9, 0.9, 1]\n",
    "# ff2 = [0.8, 1, 1, 0.9, 1, 1, 1, 1, 0.8, 0.9]\n",
    "\n",
    "t2 = [0.45]\n",
    "n2 = [700]\n",
    "max_depth2 = [2]\n",
    "models2 = [0]\n",
    "bf2 = [0.8]\n",
    "ff2 = [1]\n",
    "\n",
    "models2 = train(train_data2, t2, n2, max_depth2, bf2, ff2, models2)\n",
    "pickle.dump(models2, open('Dummy/models22.sav', 'wb'))\n",
    "pickle.dump(t2, open('Dummy/threshold22.sav', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
